{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing numbers by using a probability density function can be considered as a generative model. But it is not very useful as it is absolutely random.\n",
    "\n",
    "A more useful generative model would use data to learn from, that is to say, it will try to find, or more likely, to approximate the distribution $p_{data}$ that has been used to produce the data.\n",
    "\n",
    "When working in one dimension, the solution is fairly easy. Simply find the probability of every value in the dataset, find an approximation $p_{model}$ of the density function that generated the data (by assuming the data comes from a Gaussian model, it means computing the mean and the standard deviation) and finally generate new values that are different from the dataset but not too different because they have been generated by using the same denisty function.\n",
    "\n",
    "In multiple dimensions, the problem gets more complicated. If the data is in $N = 3$ dimensions (vectors of size 3), with 10 possible values for the first dimension, 5 for the second and 2 for the third, there are $10 \\times 5 \\times 2 = 100$ different combinations of these \"features\". (It is possible to visualize the sample space as a cube of dimension 10x5x2). This means that our generative model has $d = 100 - 1 = 99$ parameters ${\\hat{\\theta}}_{i \\in [1,99]}$, each one being the maximum likelyhood estimate of a vector from the sample space: ${\\hat{\\theta}}_{i} = \\frac{n_{i}}{D}$ where $n_{i}$ is the number of time the observation $i$ appears in the dataset and $D$ is the size of the dataset. (The maximum likelyhood estimate of the last vector can be deduced from the others).\n",
    "\n",
    "This model would not be good at generating new observations because the values which are not in the dataset (the ones the model should be able to generate) have a maximum likelyhood estimation equal to 0. To counter that, it is possible to use additive smoothing: ${\\hat{\\theta}}_{i} = \\frac{n_{i} + 1}{D + d}$\n",
    "\n",
    "Eventhough this model would be closer to a generative model, it would still not be satisfactory because every value out of the dataset has the same probability. If the dataset contained pictures, the model would generate with the same probability a random set of pixels or something that looks like a picture...\n",
    "\n",
    "If we use the naive assumption: $\\forall k,l \\in [1,N], p(x_k|x_l) = p(x_k)$ In other words, each feature $x_k$ of a vector $x$ is independant of every other feature $x_l$. By using the chain rule: $p(x) = p(x_1,...,x_N) = p(x_2,...,x_N)p(x_1) = p(x_3,...,x_N)p(x_2|x_1)p(x_1) = ... = \\prod_{k=1}^{N} p(x_k|x_1,...,x_{k-1}) = \\prod_{k=1}^{N} p(x_k)$ In this model called the Naive Bayes model, every feature is independant from the others. It is possible to estimate the coefficients $\\theta_{kl} = p(x_k = l)$ with $\\hat{\\theta_{kl}} = \\frac{n_kl}{D}$ (where $\\hat{\\theta_{kl}}$ is the number of time the feature $k$ takes the value $l$ in the dataset of size $D$) for each feature then multiply them to find the porbability of a vector. With the previous example, the model only has $d = 10 + 5 + 2 - 3 = 14$ parameters and the full density function is known as it is possible to compute th probability of any vecotr in the sample space.\n",
    "\n",
    "It is also possible to generate vectors by generating multiple 1D values (independantly) as explained at the beginning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

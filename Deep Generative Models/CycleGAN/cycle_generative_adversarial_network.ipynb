{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CycleGAN (aka: Cycle-consistent Generative Adversarial Network) is different from the other GANs. Instead of generating new observations from scratch (by giving so noise to the genrator), CycleGAN implement a method called *style transfer* that consists in creating a new observation with two inputs (a style input and an input to apply the style on).\n",
    "\n",
    "It works by simultaneously transfer the style of one type of images to another type of images. There are 3 key points in this learning process:\n",
    "- It learns how to modify one type of images with the style of the other (Validity)\n",
    "- The same method should be able to correct what happened to images during the previous step (ie: remove the other style) (Reconstruction)\n",
    "- The same method should not modify an image of the same style (ie: applying the style of an image to the same image should not modify the image) (Identity)\n",
    "\n",
    "Contrary to other style transfer methods like pix2pix, a CycleGAN does not need paired images to be trained (ie: a dataset with original images and the same images with a new style applied) because it is made of 4 models (2 generators and 2 discriminators) that will be trained in an adversarial way:\n",
    "- A generator G_AB able to convert images from style A to style B\n",
    "- A generator G_BA able to convert images from style B to style A\n",
    "- A discriminator d_A able to identify real images with style A and images generated by G_BA\n",
    "- A discriminator d_B able to identify real images with style B and images generated by G_AB\n",
    "\n",
    "Note: the discriminators do not output a binary output (unlike ther other GANs) instead it returns a tensor indicating if each part of the sample has the correct style. This comes from a architecture called PatchGAN where the discriminator divides each image in patches and guess if each patch is real or generated. In the case of a CycleGAN, this helps the discriminator focusing on the style rather than the content.\n",
    "\n",
    "Note: This type of GAN requires a dataset of images to transform and style images. Applying the style of an image to another single image can be done using Neural Style Transfer.\n",
    "\n",
    "It is really easy to create an CycleGAN using Keras Model Subclassing API as show in this [Tensorflow tutorial](https://www.tensorflow.org/tutorials/generative/cyclegan).\n",
    "\n",
    "Let's build an WGAN-GP \"from scratch\" to have a better understanding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand Made CycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, UpSampling2D, Activation, Dropout, LayerNormalization, Concatenate, LeakyReLU, Add\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback, LearningRateScheduler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# Clear TensorFlow session\n",
    "K.clear_session()\n",
    "\n",
    "# Disable eager execution\n",
    "# from tensorflow.python.framework.ops import disable_eager_execution\n",
    "# disable_eager_execution()\n",
    "\n",
    "# Tensorflow debugging\n",
    "# tf.debugging.enable_check_numerics()\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_A = np.asarray(Image.open(\"inferno.png\"))\n",
    "\n",
    "print(type(img_A))\n",
    "print(img_A.shape)\n",
    "\n",
    "# Resize\n",
    "img_A = img_A[:-8]\n",
    "print(img_A.shape)\n",
    "\n",
    "plt.imshow(img_A)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_B = np.asarray(Image.open(\"italy.jpg\"))\n",
    "\n",
    "print(type(img_B))\n",
    "print(img_B.shape)\n",
    "\n",
    "# Resize\n",
    "img_B = img_B[:-8]\n",
    "print(img_B.shape)\n",
    "\n",
    "plt.imshow(img_B)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 1920\n",
    "IMG_HEIGHT = 1072 # 1072 is a multiple of 16 which is required for this specific U-Net architecture\n",
    "IMG_DEPTH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_A = np.array([img_A])\n",
    "X_train_B = np.array([img_B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator (U-Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator_U():\n",
    "\n",
    "    def __init__(self, input_dim, generator_n_filters, model_name):\n",
    "        self.input_dim = input_dim\n",
    "        self.generator_n_filters = generator_n_filters\n",
    "\n",
    "        def downsample(layer_input, filters, kernel_size, name_suffix):\n",
    "            d = Conv2D(filters=filters,\n",
    "                       kernel_size=kernel_size,\n",
    "                       strides=2,\n",
    "                       padding=\"same\",\n",
    "                       name=\"generator_conv_\" + name_suffix)(layer_input)\n",
    "            d = LayerNormalization(axis=-1,\n",
    "                                   center=False,\n",
    "                                   scale=False,\n",
    "                                   name=\"generator_instance_normalization_\" + name_suffix)(d)\n",
    "            d = Activation(activation=\"relu\",\n",
    "                           name=\"generator_activation_\" + name_suffix)(d)\n",
    "            return d\n",
    "        \n",
    "        def upsample(layer_input, layer_skip_input, filters, kernel_size, dropout_rate, name_suffix):\n",
    "            u = UpSampling2D(size=2,\n",
    "                             name=\"generator_upsampling_\" + name_suffix)(layer_input)\n",
    "            u = Conv2D(filters=filters,\n",
    "                       kernel_size=kernel_size,\n",
    "                       strides=1,\n",
    "                       padding=\"same\",\n",
    "                       name=\"generator_conv_\" + name_suffix)(u)\n",
    "            u = LayerNormalization(axis=-1,\n",
    "                                   center=False,\n",
    "                                   scale=False,\n",
    "                                   name=\"generator_instance_normalization_\" + name_suffix)(u)\n",
    "            u = Activation(activation=\"relu\",\n",
    "                           name=\"generator_activation_\" + name_suffix)(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(rate=dropout_rate,\n",
    "                            name=\"generator_dropout_\" + name_suffix)(u)\n",
    "                \n",
    "            u = Concatenate(name=\"generator_concatenate_\" + name_suffix)([u, layer_skip_input])\n",
    "            return u\n",
    "        \n",
    "        # Input\n",
    "        self.input = Input(shape=self.input_dim,\n",
    "                           name=\"generator_input\")\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = downsample(self.input, self.generator_n_filters, 4, \"1\")\n",
    "        d2 = downsample(d1, self.generator_n_filters*2, 4, \"2\")\n",
    "        d3 = downsample(d2, self.generator_n_filters*4, 4, \"3\")\n",
    "        d4 = downsample(d3, self.generator_n_filters*8, 4, \"4\")\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = upsample(d4, d3, self.generator_n_filters*4, 4, 0, \"5\")\n",
    "        u2 = upsample(u1, d2, self.generator_n_filters*2, 4, 0, \"6\")\n",
    "        u3 = upsample(u2, d1, self.generator_n_filters, 4, 0, \"7\")\n",
    "        u4 = UpSampling2D(size=2,\n",
    "                          name=\"generator_upsampling_8\")(u3)\n",
    "        \n",
    "        # Output\n",
    "        self.output = Conv2D(filters=self.input_dim[2],\n",
    "                             kernel_size=4,\n",
    "                             strides=1,\n",
    "                             padding=\"same\",\n",
    "                             activation=\"tanh\",\n",
    "                             name=\"generator_output\")(u4)\n",
    "        \n",
    "        # Model\n",
    "        self.model = Model(self.input, self.output, name=model_name)\n",
    "        \n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator (ResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator_R():\n",
    "\n",
    "    def __init__(self, input_dim, generator_n_filters, model_name):\n",
    "        self.input_dim = input_dim\n",
    "        self.generator_n_filters = generator_n_filters\n",
    "\n",
    "        def downsample(layer_input, filters, kernel_size, name_suffix):\n",
    "            d = Conv2D(filters=filters,\n",
    "                       kernel_size=kernel_size,\n",
    "                       strides=2,\n",
    "                       padding=\"same\",\n",
    "                       name=\"generator_down_sample_conv_\" + name_suffix)(layer_input)\n",
    "            d = LayerNormalization(axis=-1,\n",
    "                                   center=False,\n",
    "                                   scale=False,\n",
    "                                   name=\"generator_down_sample_instance_normalization_\" + name_suffix)(d)\n",
    "            d = Activation(activation=\"relu\",\n",
    "                           name=\"generator_down_sample_activation_\" + name_suffix)(d)\n",
    "            return d\n",
    "        \n",
    "        def residual(layer_input, filters, name_suffix):\n",
    "            shortcut = layer_input\n",
    "            y = Conv2D(filters=filters,\n",
    "                       kernel_size=(3,3),\n",
    "                       strides=1,\n",
    "                       padding=\"same\",\n",
    "                       name=\"generator_residual_conv_1_\" + name_suffix)(layer_input)\n",
    "            y = LayerNormalization(axis=-1,\n",
    "                                   center=False,\n",
    "                                   scale=False,\n",
    "                                   name=\"generator_residual_instance_normalization_1_\" + name_suffix)(y)\n",
    "            y = Activation(activation=\"relu\",\n",
    "                           name=\"generator_residual_activation_\" + name_suffix)(y)\n",
    "            y = Conv2D(filters=filters,\n",
    "                       kernel_size=(3,3),\n",
    "                       strides=1,\n",
    "                       padding=\"same\",\n",
    "                       name=\"generator_residual_conv_2_\" + name_suffix)(y)\n",
    "            y = LayerNormalization(axis=-1,\n",
    "                                   center=False,\n",
    "                                   scale=False,\n",
    "                                   name=\"generator_residual_instance_normalization_2_\" + name_suffix)(y)\n",
    "            return Add()([shortcut, y])\n",
    "        \n",
    "        def upsample(layer_input, layer_skip_input, filters, kernel_size, dropout_rate, name_suffix):\n",
    "            u = UpSampling2D(size=2,\n",
    "                             name=\"generator_up_sample_upsampling_\" + name_suffix)(layer_input)\n",
    "            u = Conv2D(filters=filters,\n",
    "                       kernel_size=kernel_size,\n",
    "                       strides=1,\n",
    "                       padding=\"same\",\n",
    "                       name=\"generator_up_sample_conv_\" + name_suffix)(u)\n",
    "            u = LayerNormalization(axis=-1,\n",
    "                                   center=False,\n",
    "                                   scale=False,\n",
    "                                   name=\"generator_up_sample_instance_normalization_\" + name_suffix)(u)\n",
    "            u = Activation(activation=\"relu\",\n",
    "                           name=\"generator_up_sample_activation_\" + name_suffix)(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(rate=dropout_rate,\n",
    "                            name=\"generator_up_sample_dropout_\" + name_suffix)(u)\n",
    "                \n",
    "            u = Concatenate(name=\"generator_up_sample_concatenate_\" + name_suffix)([u, layer_skip_input])\n",
    "            return u\n",
    "        \n",
    "        # Input\n",
    "        self.input = Input(shape=self.input_dim,\n",
    "                           name=\"generator_input\")\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = downsample(self.input, self.generator_n_filters, 4, \"1\")\n",
    "        d2 = downsample(d1, self.generator_n_filters*2, 4, \"2\")\n",
    "        d3 = downsample(d2, self.generator_n_filters*4, 4, \"3\")\n",
    "        d4 = downsample(d3, self.generator_n_filters*8, 4, \"4\")\n",
    "\n",
    "        # Residual\n",
    "        r1 = residual(d4, self.generator_n_filters*8, \"1\")\n",
    "        r2 = residual(r1, self.generator_n_filters*8, \"2\")\n",
    "        r3 = residual(r2, self.generator_n_filters*8, \"3\")\n",
    "        r4 = residual(r3, self.generator_n_filters*8, \"4\")\n",
    "        r5 = residual(r4, self.generator_n_filters*8, \"5\")\n",
    "        r6 = residual(r5, self.generator_n_filters*8, \"6\")\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = upsample(d4, d3, self.generator_n_filters*4, 4, 0, \"5\")\n",
    "        u2 = upsample(u1, d2, self.generator_n_filters*2, 4, 0, \"6\")\n",
    "        u3 = upsample(u2, d1, self.generator_n_filters, 4, 0, \"7\")\n",
    "        u4 = UpSampling2D(size=2,\n",
    "                          name=\"generator_upsampling_8\")(u3)\n",
    "        \n",
    "        # Output\n",
    "        self.output = Conv2D(filters=self.input_dim[2],\n",
    "                             kernel_size=4,\n",
    "                             strides=1,\n",
    "                             padding=\"same\",\n",
    "                             activation=\"tanh\",\n",
    "                             name=\"generator_output\")(u4)\n",
    "        \n",
    "        # Model\n",
    "        self.model = Model(self.input, self.output, name=model_name)\n",
    "        \n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator():\n",
    "\n",
    "    def __init__(self, input_dim, discriminator_n_filters, model_name):\n",
    "        self.input_dim = input_dim\n",
    "        self.discriminator_n_filters = discriminator_n_filters\n",
    "\n",
    "        def conv4(layer_input, filters, strides, instance_normalisation, name_suffix):\n",
    "            y = Conv2D(filters=filters,\n",
    "                       kernel_size=4,\n",
    "                       strides=strides,\n",
    "                       padding=\"same\",\n",
    "                       name=\"discriminator_conv_\" + name_suffix)(layer_input)\n",
    "            \n",
    "            if instance_normalisation:\n",
    "                y = LayerNormalization(axis=-1,\n",
    "                                       center=False,\n",
    "                                       scale=False,\n",
    "                                       name=\"discriminator_instance_normalization_\" + name_suffix)(y)\n",
    "                \n",
    "            y = LeakyReLU(alpha=0.2,\n",
    "                          name=\"discriminator_leaky_relu_\" + name_suffix)(y)\n",
    "            return y\n",
    "        \n",
    "        # Input\n",
    "        self.input = Input(shape=self.input_dim)\n",
    "\n",
    "        # Layers\n",
    "        y = conv4(self.input, self.discriminator_n_filters, 2, False, \"1\")\n",
    "        y = conv4(y, self.discriminator_n_filters*2, 2, True, \"2\")\n",
    "        y = conv4(y, self.discriminator_n_filters*4, 2, True, \"3\")\n",
    "        y = conv4(y, self.discriminator_n_filters*8, 1, True, \"4\")\n",
    "\n",
    "        # Output\n",
    "        self.output = Conv2D(filters=1,\n",
    "                             kernel_size=4,\n",
    "                             strides=1,\n",
    "                             padding=\"same\",\n",
    "                             name=\"discriminator_output\")(y)\n",
    "        \n",
    "        # Model\n",
    "        self.model = Model(self.input, self.output, name=model_name)\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN():\n",
    "\n",
    "    def __init__(self, input_dim, generator_n_filtres, discriminator_n_filters, learning_rate, lambda_validation, lambda_reconstruction, lambda_identity):\n",
    "        self.input_dim = input_dim\n",
    "        self.generator_n_filtres = generator_n_filtres\n",
    "        self.discriminator_n_filters = discriminator_n_filters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_validation = lambda_validation\n",
    "        self.lambda_reconstruction = lambda_reconstruction\n",
    "        self.lambda_identity = lambda_identity\n",
    "\n",
    "        self.epoch = 0\n",
    "\n",
    "        self.discriminator_A_loss = []\n",
    "        self.discriminator_B_loss = []\n",
    "        self.discriminator_loss = []\n",
    "        self.global_loss = []\n",
    "\n",
    "        # Discriminators\n",
    "        self.discriminator_A = Discriminator(self.input_dim, self.discriminator_n_filters, model_name=\"discriminator_A\")\n",
    "        self.discriminator_A.model.compile(loss=\"mse\",\n",
    "                                           optimizer=Adam(learning_rate=self.learning_rate, beta_1=0.5),\n",
    "                                           metrics=[\"accuracy\"])\n",
    "        self.discriminator_B = Discriminator(self.input_dim, self.discriminator_n_filters, model_name=\"discriminator_B\")\n",
    "        self.discriminator_B.model.compile(loss=\"mse\",\n",
    "                                           optimizer=Adam(learning_rate=self.learning_rate, beta_1=0.5),\n",
    "                                           metrics=[\"accuracy\"])\n",
    "        \n",
    "        # Generators\n",
    "        # self.generator_AB = Generator_U(self.input_dim, self.generator_n_filtres, model_name=\"generator_AB\")\n",
    "        # self.generator_BA = Generator_U(self.input_dim, self.generator_n_filtres, model_name=\"generator_BA\")\n",
    "        self.generator_AB = Generator_R(self.input_dim, self.generator_n_filtres, model_name=\"generator_AB\")\n",
    "        self.generator_BA = Generator_R(self.input_dim, self.generator_n_filtres, model_name=\"generator_BA\")\n",
    "        \n",
    "        self.discriminator_A.model.trainable = False\n",
    "        self.discriminator_B.model.trainable = False\n",
    "\n",
    "        self.real_A = Input(shape=self.input_dim, name=\"\")\n",
    "        self.real_B = Input(shape=self.input_dim, name=\"\")\n",
    "\n",
    "        self.generated_A = self.generator_BA.model(self.real_B)\n",
    "        self.generated_B = self.generator_AB.model(self.real_A)\n",
    "\n",
    "        self.valid_A = self.discriminator_A.model(self.generated_A)\n",
    "        self.valid_B = self.discriminator_B.model(self.generated_B)\n",
    "\n",
    "        self.reconstruct_A = self.generator_BA.model(self.generated_B)\n",
    "        self.reconstruct_B = self.generator_AB.model(self.generated_A)\n",
    "\n",
    "        self.identity_A = self.generator_BA.model(self.real_A)\n",
    "        self.identity_B = self.generator_AB.model(self.real_B)\n",
    "\n",
    "        self.model = Model(inputs=[self.real_A, self.real_B],\n",
    "                           outputs=[self.valid_A, self.valid_B,\n",
    "                                    self.reconstruct_A, self.reconstruct_B,\n",
    "                                    self.identity_A, self.identity_B])\n",
    "        self.model.compile(loss=[\"mse\", \"mse\",\n",
    "                                 \"mae\", \"mae\",\n",
    "                                 \"mae\", \"mae\"],\n",
    "                           loss_weights=[self.lambda_validation, self.lambda_validation,\n",
    "                                         self.lambda_reconstruction, self.lambda_reconstruction,\n",
    "                                         self.lambda_identity, self.lambda_identity],\n",
    "                           optimizer=Adam(learning_rate=self.learning_rate, beta_1=0.5))\n",
    "        \n",
    "        self.discriminator_A.model.trainable = True\n",
    "        self.discriminator_B.model.trainable = True\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "    def plot_model(self, run_folder):\n",
    "        plot_model(self.model, to_file=os.path.join(run_folder ,'viz/model.png'), show_shapes=True, show_layer_names=True)\n",
    "        plot_model(self.discriminator_A.model, to_file=os.path.join(run_folder ,'viz/discriminator_A.png'), show_shapes=True, show_layer_names=True)\n",
    "        plot_model(self.discriminator_B.model, to_file=os.path.join(run_folder ,'viz/discriminator_B.png'), show_shapes=True, show_layer_names=True)\n",
    "        plot_model(self.generator_AB.model, to_file=os.path.join(run_folder ,'viz/generator_AB.png'), show_shapes=True, show_layer_names=True)\n",
    "        plot_model(self.generator_BA.model, to_file=os.path.join(run_folder ,'viz/generator_BA.png'), show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    def fit(self, X_train_A, X_train_B, batch_size, epochs):\n",
    "        patch_height = int(self.input_dim[0] / 2**3)\n",
    "        patch_width = int(self.input_dim[1] / 2**3)\n",
    "        patch_dim = (patch_height, patch_width, 1)\n",
    "        print(f\"patch_dim = {patch_dim}\")\n",
    "\n",
    "        real = np.ones((batch_size,) + patch_dim)       # One response per patch\n",
    "        generated = np.zeros((batch_size,) + patch_dim) # One response per patch\n",
    "\n",
    "        for epoch in range(self.epoch, epochs):\n",
    "            generated_A = self.generator_BA.predict(X_train_B)\n",
    "            generated_B = self.generator_AB.predict(X_train_A)\n",
    "\n",
    "            # Train discriminator A\n",
    "            discriminator_A_loss_real = self.discriminator_A.model.train_on_batch(X_train_A, real)\n",
    "            discriminator_A_loss_generated = self.discriminator_A.model.train_on_batch(generated_A, generated)\n",
    "            discriminator_A_loss = 0.5 * np.add(discriminator_A_loss_real, discriminator_A_loss_generated)\n",
    "            self.discriminator_A_loss.append([discriminator_A_loss_real, discriminator_A_loss_generated, discriminator_A_loss])\n",
    "\n",
    "            # Train discriminator B\n",
    "            discriminator_B_loss_real = self.discriminator_B.model.train_on_batch(X_train_B, real)\n",
    "            discriminator_B_loss_generated = self.discriminator_B.model.train_on_batch(generated_B, generated)\n",
    "            discriminator_B_loss = 0.5 * np.add(discriminator_B_loss_real, discriminator_B_loss_generated)\n",
    "            self.discriminator_B_loss.append([discriminator_B_loss_real, discriminator_B_loss_generated, discriminator_B_loss])\n",
    "\n",
    "            discriminator_loss = 0.5 * np.add(discriminator_A_loss, discriminator_B_loss)\n",
    "            self.discriminator_loss.append(discriminator_loss)\n",
    "\n",
    "            # Train generators\n",
    "            g_loss = self.model.train_on_batch([X_train_A, X_train_B],\n",
    "                                                        [real, real,\n",
    "                                                         X_train_A, X_train_B,\n",
    "                                                         X_train_A, X_train_B])\n",
    "            self.global_loss.append(g_loss)\n",
    "            \n",
    "            self.epoch += 1\n",
    "\n",
    "    def load_weights(self, filepath=\"model/weights/params.pkl\"):\n",
    "        self.model.load_weights(filepath)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.generator.model.predict(x)\n",
    "\n",
    "    def save(self, folder=\"model\"):\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "            os.makedirs(os.path.join(folder, 'viz'))\n",
    "            os.makedirs(os.path.join(folder, 'weights'))\n",
    "            os.makedirs(os.path.join(folder, 'images'))\n",
    "\n",
    "        with open(os.path.join(folder, 'weights/params.pkl'), 'wb') as f:\n",
    "            pickle.dump([self.input_dim,\n",
    "                         self.discriminator_conv_filters,\n",
    "                         self.discriminator_conv_kernel_sizes,\n",
    "                         self.discriminator_conv_strides,\n",
    "                         self.discriminator_batch_norm_momentum,\n",
    "                         self.discriminator_dropout_rate,\n",
    "                         self.latent_dim,\n",
    "                         self.generator_conv_filters,\n",
    "                         self.generator_conv_kernel_sizes,\n",
    "                         self.generator_conv_strides,\n",
    "                         self.generator_batch_norm_momentum,\n",
    "                         self.generator_dropout_rate], f)\n",
    "        self.plot_model(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_gan = CycleGAN(input_dim=(IMG_HEIGHT,IMG_WIDTH,IMG_DEPTH),\n",
    "                     generator_n_filtres=32,\n",
    "                     discriminator_n_filters=32,\n",
    "                     learning_rate=0.0002,\n",
    "                     lambda_validation=1,\n",
    "                     lambda_reconstruction=10,\n",
    "                     lambda_identity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_gan.plot_model(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_gan.discriminator_A.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycle_gan.discriminator_B.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_gan.generator_AB.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycle_gan.generator_BA.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_gan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_gan.fit(X_train_A, X_train_B, BATCH_SIZE, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate CycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot discriminator losses\n",
    "plt.plot([row[0] for row in cycle_gan.discriminator_losses], label=\"loss\")\n",
    "plt.plot([row[1] for row in cycle_gan.discriminator_losses], label=\"loss (real images)\")\n",
    "plt.plot([row[2] for row in cycle_gan.discriminator_losses], label=\"loss (generated images)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_gan.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycle_gan.load_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

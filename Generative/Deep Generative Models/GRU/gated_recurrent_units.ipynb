{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Recurrent Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Gated Recurrent Units (GRU) network is a Recurent Neural Network (RNN) that has the capacity to remember.\n",
    "\n",
    "Note: Biderectionnal GRU cells can be used when the entire text in available to the model at inference.\n",
    "\n",
    "Let's build a GRU network \"from scratch\" to have a better understanding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand Made GRU Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, GRU, Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import Callback, LearningRateScheduler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# Clear TensorFlow session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !bash data_script.sh 11339 aesop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, start_story):\n",
    "    text = text.lower()\n",
    "    text = start_story + text\n",
    "    text = text.replace(\"\\n\\n\\n\\n\\n\", start_story)\n",
    "    text = text.replace(\"\\n\", \" \")   # Replace newline character by a space\n",
    "    text = re.sub(\"  +\", \". \", text) # Replace space followed by oen or more space by a point and a space\n",
    "    text = text.strip()              # Remove leading and trailing spaces\n",
    "    text = text.replace(\"..\", \".\")   # Replace double point by a single point\n",
    "    text = re.sub('([!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~])', r' \\1', text)\n",
    "    text = re.sub(\"\\s{2,}\", \" \", text) # Replace two whitespace character by one space\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"./data/aesop/data.txt\"\n",
    "\n",
    "SEQ_LENGTH = 20\n",
    "START_STORY = \"|\" * SEQ_LENGTH\n",
    "\n",
    "with open(FILE_PATH, encoding=\"utf-8-sig\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Preprocessing\n",
    "text = preprocess(text, START_STORY)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(char_level=False,\n",
    "                      filters=\"\")\n",
    "tokenizer.fit_on_texts([text])\n",
    "NB_TOTAL_WORDS = len(tokenizer.word_index) + 1\n",
    "token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "print(f\"Total words = {NB_TOTAL_WORDS}\")\n",
    "print(f\"Tokenizer word index = {tokenizer.word_index}\")\n",
    "print(f\"Tokenizer token list = {tokenizer.texts_to_sequences([text])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Tokenizer token list = {tokenizer.texts_to_sequences([\"man the lion\"])}')\n",
    "print(f'Pre-processed text sample = {preprocess(\"man the lion\", START_STORY)}')\n",
    "print(f'Tokenizer token list = {tokenizer.texts_to_sequences([preprocess(\"man the lion\", START_STORY)])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(token_list, sequence_length, step, nb_classes):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(0, len(token_list) - sequence_length, step):\n",
    "        X.append(token_list[i:i+sequence_length])\n",
    "        y.append(token_list[i+sequence_length])\n",
    "\n",
    "    y = tf.keras.utils.to_categorical(y, num_classes=nb_classes)\n",
    "\n",
    "    nb_seq = len(X)\n",
    "    print(f\"Number of sequences = {nb_seq}\")\n",
    "\n",
    "    return X, y, nb_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset\n",
    "STEP = 1\n",
    "\n",
    "X, y, NB_SEQ = generate_sequences(token_list, SEQ_LENGTH, STEP, NB_TOTAL_WORDS)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_RNN():\n",
    "\n",
    "    def __init__(self, nb_units, embedding_size, total_words, dropout_rate=0.2,learning_rate=0.001):\n",
    "        self.nb_units = nb_units\n",
    "        self.embedding_size = embedding_size\n",
    "        self.total_words = total_words\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Input\n",
    "        self.input = Input(shape=(None,))\n",
    "\n",
    "        # Layers\n",
    "        x = Embedding(input_dim=self.total_words,\n",
    "                      output_dim=self.embedding_size,\n",
    "                      name=\"embedding\")(self.input)\n",
    "        x = GRU(units=self.nb_units,\n",
    "                 name=\"gru\")(x)\n",
    "        x = Dropout(rate=dropout_rate,\n",
    "                    name=\"dropout\")(x)\n",
    "        \n",
    "        # Output\n",
    "        self.output = Dense(units=self.total_words,\n",
    "                            activation=\"softmax\",\n",
    "                            name=\"output\")(x)\n",
    "        \n",
    "        # Model\n",
    "        self.model = Model(self.input, self.output)\n",
    "\n",
    "        # Compile\n",
    "        self.optimizer = RMSprop(learning_rate=self.learning_rate)\n",
    "        self.model.compile(optimizer=self.optimizer,\n",
    "                           loss=\"categorical_crossentropy\")\n",
    "        \n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "    def plot_model(self, file_path=\"model/viz/model.png\"):\n",
    "        plot_model(self.model, to_file=file_path, show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    def load_weights(self, file_path=\"./model/weights/save\"):\n",
    "        self.model.load_weights(file_path)\n",
    "        \n",
    "    def fit(self, X_train, y_train, batch_size=32, epochs=100):\n",
    "        self.model.fit(x=X_train,\n",
    "                       y=y_train,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=epochs,\n",
    "                       shuffle=True)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n",
    "    \n",
    "    def save_weights(self, file_path=\"./model/weights/save\"):\n",
    "        self.model.save_weights(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_UNITS = 256\n",
    "EMBEDDING_SIZE = 100\n",
    "DROPOUT_RATE = 0.2\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "gru = GRU_RNN(nb_units=NB_UNITS,\n",
    "              embedding_size=EMBEDDING_SIZE,\n",
    "              total_words=NB_TOTAL_WORDS,\n",
    "              dropout_rate=DROPOUT_RATE,\n",
    "              learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru.plot_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked-LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stacked_GRU_RNN():\n",
    "\n",
    "    def __init__(self, nb_units, embedding_size, total_words, dropout_rate=0.2,learning_rate=0.001):\n",
    "        self.nb_units = nb_units\n",
    "        self.embedding_size = embedding_size\n",
    "        self.total_words = total_words\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Input\n",
    "        self.input = Input(shape=(None,))\n",
    "\n",
    "        # Layers\n",
    "        x = Embedding(input_dim=self.total_words,\n",
    "                      output_dim=self.embedding_size,\n",
    "                      name=\"embedding\")(self.input)\n",
    "        x = GRU(units=self.nb_units,\n",
    "                 return_sequences=True,\n",
    "                 name=\"gru_1\")(x)\n",
    "        x = GRU(units=self.nb_units,\n",
    "                 name=\"gru_2\")(x)\n",
    "        x = Dropout(rate=dropout_rate,\n",
    "                    name=\"dropout\")(x)\n",
    "        \n",
    "        # Output\n",
    "        self.output = Dense(units=self.total_words,\n",
    "                            activation=\"softmax\",\n",
    "                            name=\"output\")(x)\n",
    "        \n",
    "        # Model\n",
    "        self.model = Model(self.input, self.output)\n",
    "\n",
    "        # Compile\n",
    "        self.optimizer = RMSprop(learning_rate=self.learning_rate)\n",
    "        self.model.compile(optimizer=self.optimizer,\n",
    "                           loss=\"categorical_crossentropy\")\n",
    "        \n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "    def plot_model(self, file_path=\"model/viz/model.png\"):\n",
    "        plot_model(self.model, to_file=file_path, show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    def load_weights(self, file_path=\"./model/weights/save\"):\n",
    "        self.model.load_weights(file_path)\n",
    "        \n",
    "    def fit(self, X_train, y_train, batch_size=32, epochs=100):\n",
    "        self.model.fit(x=X_train,\n",
    "                       y=y_train,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=epochs,\n",
    "                       shuffle=True)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)\n",
    "    \n",
    "    def save_weights(self, file_path=\"./model/weights/save\"):\n",
    "        self.model.save_weights(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_UNITS = 256\n",
    "EMBEDDING_SIZE = 100\n",
    "DROPOUT_RATE = 0.2\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "stacked_gru = Stacked_GRU_RNN(nb_units=NB_UNITS,\n",
    "                              embedding_size=EMBEDDING_SIZE,\n",
    "                              total_words=NB_TOTAL_WORDS,\n",
    "                              dropout_rate=DROPOUT_RATE,\n",
    "                              learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_gru.plot_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_gru.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "gru.fit(X, y, BATCH_SIZE, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 4\n",
    "\n",
    "stacked_gru.fit(X, y, BATCH_SIZE, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate LSTM Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru.save_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_gru.save_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_gru.load_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temp(preds, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample an index from a probability array.\n",
    "    \"\"\"\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probs = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(tokenizer, model, seed_text, nb_words, max_sequence_length, temperature):\n",
    "    output_text = seed_text\n",
    "    seed_text = preprocess(seed_text, START_STORY)\n",
    "    seed_text = seed_text\n",
    "    print(f\"seed_text = {seed_text}\")\n",
    "\n",
    "    for i in range(nb_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = token_list[-max_sequence_length:] # take the last max_sequence_length tokens (cap the input sequence to improve performance)\n",
    "        token_list = np.reshape(token_list, (1, max_sequence_length))\n",
    "\n",
    "        probs = model.predict(token_list, verbose=0)[0]\n",
    "        y_class = sample_with_temp(probs, temperature)\n",
    "\n",
    "        output_word = tokenizer.index_word[y_class] if y_class > 0 else \"\"\n",
    "\n",
    "        if output_word == \"|\":\n",
    "            break\n",
    "\n",
    "        seed_text += output_word + \" \"\n",
    "        output_text += output_word + \" \"\n",
    "\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed text\n",
    "seed_text = \"the lion and the man\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = generate_text(tokenizer=tokenizer,\n",
    "                            model=gru.model,\n",
    "                            seed_text=seed_text,\n",
    "                            nb_words=100,\n",
    "                            max_sequence_length=20,\n",
    "                            temperature=0.2)\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = generate_text(tokenizer=tokenizer,\n",
    "                            model=stacked_gru.model,\n",
    "                            seed_text=seed_text,\n",
    "                            nb_words=100,\n",
    "                            max_sequence_length=20,\n",
    "                            temperature=0.2)\n",
    "\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
